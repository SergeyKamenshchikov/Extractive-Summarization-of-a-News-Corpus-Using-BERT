{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "The data wrangling process goes through 8 stages:\n",
    "\n",
    "__1.__ Load data from source jsonl files <br><br> \n",
    "__2.__ Filter data for extractive summaries only<br><br> \n",
    "__3.1__ Preprocess data with spacy to form cleaned sentence lists for each article <br><br>\n",
    "__3.2__ Use sentence tranformer to calcuate BERT sentence embeddings as the features set <br><br>\n",
    "__4.__ Calculate target variables by finding highest cosine similarity article sentence for each summary sentence <br><br> \n",
    "__5.__ Add additional features (sentence number and document length) <br><br>\n",
    "__6.__ Add subject domain labels for each article <br><br>\n",
    "__7.__ Preprocess for train_test_split on embeddings features set only with split at document level <br><br>\n",
    "__8.__ Add sentence number to embeddings features for another train_test_split set <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load data from source jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load_data_from_jsonl.py\n",
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "input_file = 'train-stats.jsonl'\n",
    "output_file = 'train_stats_df_no_spacy.pickle'\n",
    "\n",
    "#read jsonl file into list of sample rows\n",
    "counter=0\n",
    "data=[]\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "        counter +=1\n",
    "        \n",
    "#wrap in dataframe        \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#save to pickle\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(df, handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter Data\n",
    "\n",
    "Filter data for extractive summaries only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "filter_extractive_data_only.py\n",
    "\"\"\" \n",
    "import pickle\n",
    "import pandas as pd\n",
    " \n",
    "output_file = 'train_stats_df_extractive_no_spacy.pickle'\n",
    "\n",
    "#load all data\n",
    "df = pd.read_pickle('train_stats_df_no_spacy.pickle')\n",
    "\n",
    "#filter for extractive summaries only\n",
    "df = df[df.density_bin == 'extractive']\n",
    "\n",
    "#save to pickle file\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(df, handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing: Sentence Tokenization and Embeddings (Features)\n",
    "\n",
    "3.1 Preprocess data with spacy to form cleaned sentence lists for each article <br> \n",
    "3.2 Use sentence tranformer to calcuate BERT sentence embeddings as the features set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "preprocessing_embed.py\n",
    "\"\"\"\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "### Helper function\n",
    "\n",
    "def text_to_sent_list(text, \n",
    "                      nlp = spacy.load(\"en_core_web_lg\"), \n",
    "                      embedder = SentenceTransformer('distilbert-base-nli-mean-tokens'),\n",
    "                      min_len=2):\n",
    "    \n",
    "    ''' Returns cleaned article sentences and BERT sentence embeddings'''\n",
    "    \n",
    "    #convert to list of sentences\n",
    "    text = nlp(text)\n",
    "    sents = list(text.sents)\n",
    "    #remove short sentences by threshhold                                                                                                \n",
    "    sents_clean = [sentence.text for sentence in sents if len(sentence)> min_len]\n",
    "    #remove entries with empty list\n",
    "    sents_clean = [sentence for sentence in sents_clean if len(sentence)!=0]\n",
    "    #embed sentences (deafult uses BERT SentenceTransformer)\n",
    "    sents_embedding= np.array(embedder.encode(sents_clean, convert_to_tensor=True))\n",
    "    \n",
    "    return sents_clean, sents_embedding\n",
    "\n",
    "\n",
    "\n",
    "### Script\n",
    "\n",
    "output_file = 'train_stats_df_processed_extr_5000.pickle'  \n",
    "#load full extractive df\n",
    "df = pd.read_pickle('train_stats_df_extractive_no_spacy.pickle')\n",
    "\n",
    "#truncate for local computation\n",
    "df= df.head(5000).reset_index(drop=True)\n",
    "\n",
    "#load nlp and embedder\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "embedder = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "t1 = dt.now()\n",
    "print(t1)\n",
    "\n",
    "#extract clean sentence list and sentence embedding for each article TEXT\n",
    "f = lambda text: text_to_sent_list(text, nlp=nlp, embedder=embedder, min_len=2)\n",
    "s_interim_tuple = df['text'].apply(f)\n",
    "\n",
    "df['text_clean'] = s_interim_tuple.apply(lambda x: x[0])\n",
    "df['text_embedding'] = s_interim_tuple.apply(lambda x: x[1])\n",
    "\n",
    "#extract clean sentence list and sentence embedding for each article SUMMARY\n",
    "f = lambda summ: text_to_sent_list(summ, nlp=nlp, embedder=embedder, min_len=0)\n",
    "s_interim_tuple = df['summary'].apply(f)\n",
    "\n",
    "df['summary_clean'] = s_interim_tuple.apply(lambda x: x[0])\n",
    "df['summary_embedding'] = s_interim_tuple.apply(lambda x: x[1])\n",
    "\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(df, handle)\n",
    "\n",
    "t2=dt.now()\n",
    "print(t2)\n",
    "print(t2-t1)\n",
    "\n",
    "#5000 articles took 1hr40mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculating Target Labels\n",
    "\n",
    "Calculate target variables by finding highest cosine similarity article sentence for each summary sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "preprocessing label_target.py\n",
    "\"\"\"\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "def find_sim_single_summary(summary_sentence_embed, doc_emedding):\n",
    "    '''returns array of indices for max cosine sim per summary sentences'''\n",
    "    cos_sim_mat = cosine_similarity(doc_emedding, summary_sentence_embed)\n",
    "    idx_arr = np.argmax(cos_sim_mat, axis=0)\n",
    "    \n",
    "    return idx_arr\n",
    "\n",
    "def label_sent_in_summary(s_text, s_summary):\n",
    "    '''returns index list and binary target labels in an array'''\n",
    "    doc_num = s_text.shape[0]\n",
    "    \n",
    "    #initialize zeros\n",
    "    labels = [np.zeros(doc.shape[0]) for doc in s_text.tolist()] \n",
    "    \n",
    "    #calc idx for most similar\n",
    "    \n",
    "    idx_list = [np.sort(find_sim_single_summary(s_summary[j], s_text[j])) for j \n",
    "                                                            in range(doc_num)]\n",
    "      \n",
    "    for j in range(doc_num):\n",
    "        labels[j][idx_list[j]]= 1 \n",
    "    \n",
    "    return idx_list, labels\n",
    "\n",
    "\n",
    "### Script\n",
    "\n",
    "t1 = dt.now()\n",
    "print(t1)\n",
    "\n",
    "output_file = 'train_stats_df_processed_extr_label_5000.pickle'\n",
    "\n",
    "df = pd.read_pickle('train_stats_df_processed_extr_5000.pickle' )\n",
    "\n",
    "#get index list and target labels\n",
    "idx_list, labels = label_sent_in_summary(df.text_embedding, df.summary_embedding)\n",
    "\n",
    "#wrap in dataframe\n",
    "df['labels'] = labels\n",
    "df['labels_idx_list'] = idx_list\n",
    "\n",
    "#save to pickle\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(df, handle)\n",
    "\n",
    "t2 = dt.now()\n",
    "\n",
    "print(t2)\n",
    "\n",
    "print(t2-t1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Additional Features\n",
    "\n",
    "Add additional features (sentence number and document length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "add_additional_features.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime as dt\n",
    "\n",
    "t1 = dt.now()\n",
    "print(t1)\n",
    "    \n",
    "output_file = 'train_stats_dict_processed_extr_final_5000_.pickle'\n",
    "    \n",
    "df = pd.read_pickle('train_stats_df_processed_extr_label_5000.pickle')\n",
    "\n",
    "#define features and labels as separate series\n",
    "s_embed_text = df.text_embedding\n",
    "s_y_labels= df.labels\n",
    "\n",
    "#label docs\n",
    "s_doc_label = pd.Series(range(df.shape[0]), name = 'doc_label')\n",
    "\n",
    "#calculate doc mean\n",
    "s_doc_mean = s_embed_text.apply(lambda x: x.mean(axis=0).reshape(1,-1))\n",
    "    \n",
    "#calculate doc sent length\n",
    "s_doc_length = s_embed_text.apply(lambda x: x.shape[0])\n",
    "\n",
    "\n",
    "#create values for each sentence in doc \n",
    "X_doc_label_list =[]\n",
    "X_doc_mean_list = []\n",
    "X_doc_length_list = []\n",
    "X_sent_num_list = []\n",
    "\n",
    "for j in range(len(df)):\n",
    "    X_doc_label = s_doc_label[j]\n",
    "    X_doc_mean = s_doc_mean[j]\n",
    "    X_doc_length = s_doc_length[j]\n",
    "    X_text = s_embed_text [j]\n",
    "    n = X_text.shape[0]\n",
    "    \n",
    "    X_doc_label_fixed = X_doc_label\n",
    "    X_doc_mean_fixed = X_doc_mean\n",
    "    X_doc_length_fixed = X_doc_length \n",
    "    sent_num = []\n",
    "    for i in range(n-1): \n",
    "        X_doc_label = np.vstack((X_doc_label, X_doc_label_fixed )) \n",
    "        X_doc_mean = np.vstack((X_doc_mean, X_doc_mean_fixed )) \n",
    "        X_doc_length = np.vstack((X_doc_length, X_doc_length_fixed )) \n",
    "        sent_num.append(i)\n",
    "    sent_num.append(n-1)\n",
    "    \n",
    "    X_doc_label_list.append(X_doc_label)\n",
    "    X_doc_mean_list.append(X_doc_mean)\n",
    "    X_doc_length_list.append(X_doc_length)\n",
    "    X_sent_num_list.append(np.array(sent_num).reshape(-1,1))\n",
    "    \n",
    "#from list to pandas series\n",
    "s_doc_label = pd.Series(X_doc_label_list)\n",
    "s_doc_mean = pd.Series(X_doc_mean_list)\n",
    "s_doc_length = pd.Series(X_doc_length_list)\n",
    "s_sent_num = pd.Series(X_sent_num_list)\n",
    "\n",
    "#concatenate documents with rows = sentences\n",
    "  #intialize\n",
    "Xy_doc_label = s_doc_label.values[0]\n",
    "X = np.hstack((s_embed_text[0], s_doc_mean[0], s_sent_num[0], s_doc_length[0]))\n",
    "y= s_y_labels[0].reshape(-1,1)\n",
    "  #recursive population\n",
    "f = np.vectorize(lambda x: x if type(x) == np.ndarray else np.array([[x]]))  \n",
    "for j in range(1, len(df)):\n",
    "    Xy_doc_label_new = s_doc_label.values[j]\n",
    "    \n",
    "    X_text_new = s_embed_text [j]\n",
    "    X_sent_num_new = s_sent_num[j]\n",
    "    X_doc_mean_new = s_doc_mean[j]\n",
    "    X_doc_length_new = f(s_doc_length[j])\n",
    "    y_new = s_y_labels[j].reshape(-1,1)\n",
    "    \n",
    "    X_new = np.hstack((X_text_new, X_doc_mean_new, X_sent_num_new, X_doc_length_new))\n",
    "    \n",
    "    X = np.vstack((X, X_new))\n",
    "    y = np.vstack((y, y_new))           \n",
    "    \n",
    "    Xy_doc_label = np.vstack((Xy_doc_label, Xy_doc_label_new))\n",
    "        \n",
    "#wrap X in dataframe with lables\n",
    "labels_text_embedding = ['Sent_BERT_D_' + str(j) for j in range(768)]\n",
    "labels_doc_mean = ['Doc_BERT_D_' + str(j) for j in range(768)]\n",
    "other_labels = ['Sent_Number', 'Doc_Length']\n",
    "col_names = labels_text_embedding + labels_doc_mean + other_labels\n",
    "\n",
    "df_X = pd.DataFrame(X, columns = col_names)\n",
    "    \n",
    "data_dict = {'df_original': df, 'Xy_doc_label_array': Xy_doc_label, \n",
    "              'df_X': df_X, 'y_array': y}\n",
    "    \n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(data_dict, handle)\n",
    "    \n",
    "t2 = dt.now()\n",
    "\n",
    "print(t2)\n",
    "print(t2-t1)\n",
    "\n",
    "#5000 articles took 48 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Subject Domain Labels\n",
    "\n",
    "Add subject domain labels for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "add_domain_labels.py\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "output_file = 'extractive_all_domain_labels.pickle' \n",
    "\n",
    "input_file = 'train_stats_dict_processed_extr_final_5000_.pickle' \n",
    "data = pd.read_pickle(input_file )\n",
    "\n",
    "embedder = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "#Make single df with only Embeddings and doc label\n",
    "df_embed = data['df_X'].loc[:,'Doc_BERT_D_0': 'Doc_BERT_D_767']\n",
    "df_doc_label = pd.DataFrame(data['Xy_doc_label_array'],columns=['doc_label'])\n",
    "df = pd.concat([df_doc_label, df_embed], axis=1)\n",
    "df = df.drop_duplicates().set_index('doc_label', drop=True)\n",
    "\n",
    "#embed lambda function\n",
    "embed = lambda x: embedder.encode(x, convert_to_tensor=False)\n",
    "\n",
    "#define subject domains\n",
    "domains = ['entertainment','politics', 'business', 'crime']\n",
    "#find domain word embeddings using BERT\n",
    "domain_embed = [embed(dom) for dom in domains]\n",
    "#wrap in dataframe\n",
    "df_dom_embed = pd.DataFrame(domain_embed, index = domains,\n",
    "                            columns = df.columns)\n",
    "#calculate cosine similarity between article and each subject\n",
    "cos_matrix = cosine_similarity(df, df_dom_embed)\n",
    "\n",
    "#return subject word from index number function\n",
    "f = np.vectorize(lambda x: domains[x])\n",
    "#find max cos sim and return matching subject\n",
    "doc_domain = f(np.argmax(cos_matrix, axis=1))\n",
    "#Add to primary dataframe\n",
    "df['domain'] = doc_domain\n",
    "\n",
    "#Add to primary dictionary for storage\n",
    "data.update({'domain_labels_arr': df['domain'].values})\n",
    "\n",
    "#save to pickle file\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(data, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train_test_split 1: Embedding Features Only\n",
    "\n",
    "Preprocess for train_test_split on embeddings features set only with split at document level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Function\n",
    "\n",
    "'''functions.py'''\n",
    "\n",
    "def gen_train_test_split_doc_level(Xy_doc_label, X, y, \n",
    "                                         test_ratio, folds=1, rand_seed=42):\n",
    "    '''returns train doc labels, test doc labels, and train and test sets\n",
    "    for features X and target Y'''\n",
    "    \n",
    "    \n",
    "    random.seed(rand_seed)\n",
    "    \n",
    "    #index is doc label \n",
    "    total_docs = Xy_doc_label.max()\n",
    "    train_docs_num = int(total_docs*(1-test_ratio))\n",
    "    \n",
    "    #for k >1, want to ensure different seeds\n",
    "    rand_state_list = random.sample(range(2*folds), folds)\n",
    "    \n",
    "    #look through k folds\n",
    "    train_test_set = []\n",
    "    for state in rand_state_list:\n",
    "    \n",
    "        random.seed(state)\n",
    "        #sample random training set and mask\n",
    "        train_docs = random.sample(range(1, total_docs+1), train_docs_num)\n",
    "        train_mask = np.array([x in train_docs for x in list(Xy_doc_label)])\n",
    "        \n",
    "        #use mask to define train and test sets\n",
    "        X_train = X[train_mask]\n",
    "        y_train = y[train_mask]\n",
    "    \n",
    "        X_test = X[~train_mask]\n",
    "        y_test = y[~train_mask]\n",
    "    \n",
    "        Xy_doc_label_train = Xy_doc_label[train_mask]\n",
    "        Xy_doc_label_test = Xy_doc_label[~train_mask]\n",
    "        \n",
    "        #assign all data to tuple for each pass\n",
    "        data_pass = (Xy_doc_label_train, Xy_doc_label_test,\n",
    "                                             X_train, X_test, y_train, y_test)\n",
    "        #append results for ith fold to set \n",
    "        train_test_set.append(data_pass)\n",
    "    \n",
    "    #set answer tuples to final tuple as container\n",
    "    train_test_set = tuple(train_test_set)\n",
    "\n",
    "    return train_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "preprocess_train_test_split_embeddings_only.py\n",
    "\"\"\"\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from functions import gen_train_test_split_doc_level\n",
    "\n",
    "input_filename = 'extractive_all_domain_labels.pickle' \n",
    "folds = 1\n",
    "\n",
    "output_file = 'train_test_set20_embeddings_only.pickle'\n",
    "\n",
    "data_dict = pd.read_pickle(input_filename)\n",
    "\n",
    "#Specify model inputs: df, X, y, doc_labels\n",
    "df = data_dict['df_original']\n",
    "Xy_doc_label = data_dict['Xy_doc_label_array']\n",
    "X = data_dict['df_X'].drop(['Sent_Number','Doc_Length'], axis=1).values\n",
    "y = data_dict['y_array']\n",
    "\n",
    "        \n",
    "#train test split at document level\n",
    "\n",
    "train_test_set = gen_train_test_split_doc_level(Xy_doc_label, X, y, \n",
    "                                         test_ratio=0.2, folds=folds, rand_seed=42)\n",
    "\n",
    "data_dict.update({'train_test_sets': train_test_set })\n",
    "\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(data_dict, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train_test_split 2: Embedding and Num Sentence Features\n",
    "\n",
    "Add sentence number to embeddings features for another train_test_split set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "add_sent_num_to_train_test_split.py\n",
    "\"\"\"\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from functions import gen_train_test_split_doc_level\n",
    "\n",
    "\n",
    "input_filename = 'extractive_all_domain_labels.pickle' \n",
    "folds = 1\n",
    "\n",
    "output_file = 'train_test_set20_embeddings_sent_num.pickle'\n",
    "\n",
    "data_dict = pd.read_pickle(input_filename)\n",
    "\n",
    "#Specify model inputs: df, X, y, doc_labels\n",
    "df = data_dict['df_original']\n",
    "Xy_doc_label = data_dict['Xy_doc_label_array']\n",
    "X = data_dict['df_X'].drop(['Doc_Length'], axis=1).values\n",
    "y = data_dict['y_array']\n",
    "\n",
    "        \n",
    "#train test split at document level\n",
    "\n",
    "train_test_set = gen_train_test_split_doc_level(Xy_doc_label, X, y, \n",
    "                                         test_ratio=0.2, folds=folds, rand_seed=42)\n",
    "\n",
    "data_dict.update({'train_test_sets': train_test_set })\n",
    "\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(data_dict, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
