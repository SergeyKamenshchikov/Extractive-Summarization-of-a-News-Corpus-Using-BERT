{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "#### 1. Helper Functions\n",
    "\n",
    "There are 2 helper funcitons used throughout this section:\n",
    "\n",
    "* _return_df_pred_summaries_: returns the predicted summaries given the fixed number of sentences required \n",
    "* _calc_rouge_scores_: calculates average Rouge scores across multiple predicted and gold summary pairs\n",
    "\n",
    "#### 2. Supervised Learning Using Only Embedding Information\n",
    "\n",
    "* Logistic Regression Models\n",
    "    * Defaul model\n",
    "    * Balanced class weights\n",
    "    * Elastic Net with hyperparamter tuning <br><br>\n",
    "    \n",
    "* Neural Net Models\n",
    "    * Single layer dense network with 25 neurons\n",
    "    * Single layer dense network with 50 neurons\n",
    "    * Single layer dense network with 75 neurons\n",
    "    * Double layer dense dense network with 25 and 25 neurons\n",
    "    * Double layer dense dense network with 25 and 50 neurons\n",
    "    * Double layer dense dense network with 50 and 50 neurons<br><br>\n",
    "    \n",
    "* TextRank\n",
    "    * TextRank ranking matrix per article\n",
    "    * TextRank rouge score\n",
    "\n",
    "\n",
    "#### 3. Supervised Learning Including Sequential Information\n",
    "\n",
    "* Logistic Regression Models\n",
    "    * Defaul model\n",
    "    * Balanced class weights <br><br>\n",
    "    \n",
    "* Long Short Term Memory Models\n",
    "    * Single layer unidirectional LSTM with 25 neurons\n",
    "    * Single layer unidirectional LSTM with 50 neurons\n",
    "    * Single layer bidirectional LSTM with 25 neurons\n",
    "    * Single layer bidirectional LSTM with 50 neurons<br><br>\n",
    "\n",
    "* LEDE3\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return_df_pred_summaries_: returns the predicted summaries given the fixed number of sentences required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''functions.py'''\n",
    "\n",
    "###Sub-function used in return_pred_summaries\n",
    "\n",
    "def return_greater_than_min_num(arr, thresh=0.5, min_num=1, fix_num_flag=False, fix_num=3):\n",
    "    \n",
    "    '''returns top sentences by index numbers in ascending format and according to input\n",
    "    specifications\n",
    "    '''\n",
    "    #want fixed number sentences?\n",
    "    if fix_num_flag == True:\n",
    "        idx = np.argsort(arr)[-fix_num:]\n",
    "        \n",
    "    #return above model threshold only    \n",
    "    else:\n",
    "        idx_prelim = np.where(arr>= thresh)\n",
    "        \n",
    "        #filter for minimum number required\n",
    "        if idx_prelim[0].shape[0] <= min_num:\n",
    "            idx = np.argsort(arr)[-min_num:]\n",
    "        else:\n",
    "            idx = idx_prelim\n",
    "            \n",
    "    #return in ascending order\n",
    "    return sorted(idx)\n",
    "\n",
    "\n",
    "###Main helper function    \n",
    "def return_df_pred_summaries( Xy_doc_label, y_pred, df_text, thresh, min_num,\n",
    "                             return_all=False, fix_num_flag=False, fix_num=3):\n",
    "    \n",
    "    '''return list of predicted summaries and additional information if required\n",
    "    and according to inout specifications'''\n",
    "    \n",
    "    #Wrangle to doc label and flattened array of predictions for each article\n",
    "    df_label_pred = pd.DataFrame({'doc_label': Xy_doc_label.flatten(),\n",
    "                                                 'y_pred': y_pred.flatten()}) \n",
    "    df_label_pred = df_label_pred.groupby('doc_label').agg(list) \n",
    "\n",
    "    df_label_pred = df_label_pred.applymap(lambda x: np.array(x))\n",
    "\n",
    "    #subfunction to lambda\n",
    "    f = lambda arr: return_greater_than_min_num(arr, thresh=thresh, \n",
    "                                    min_num=min_num,fix_num_flag = fix_num_flag, \n",
    "                                                            fix_num=fix_num)\n",
    "    #get sorted index sentence numbers to include in article\n",
    "    df_label_pred = df_label_pred.applymap(f) \n",
    "\n",
    "    #Return predicted summary\n",
    "          #index is doc label\n",
    "    df_doc = df_text[df_label_pred.index]     \n",
    "    \n",
    "          # return article sentences as list\n",
    "    pred_summaries = [np.array(df_doc.iloc[j])       \n",
    "                               [df_label_pred.iloc[j][0]].tolist()                      \n",
    "                                          for j in range(len(df_label_pred))]\n",
    "          #join into summary as single string\n",
    "    pred_summaries = [summ_list if type(summ_list) == str else   \n",
    "                      ' '.join(summ_list) for summ_list in pred_summaries]  \n",
    "    \n",
    "    if return_all == True:\n",
    "        answer = df_label_pred.values, df_label_pred.index, pred_summaries\n",
    "    else:\n",
    "        answer = pred_summaries\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc_rouge_scores: calculates average Rouge scores across multiple predicted and gold summary pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supervised Learning Using Only Embedding Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.1 Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.1.1__ General Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "logreg_default_embeddings_only.py\n",
    "'''\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from functions import return_df_pred_summaries\n",
    "from functions import calc_rouge_scores\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "input_filename = 'train_test_set20_embeddings_only.pickle'\n",
    "\n",
    "output_file = 'XXX.pickle'\n",
    "\n",
    "\n",
    "t1 = dt.now()\n",
    "print(t1)\n",
    "\n",
    "data_dict = pd.read_pickle(input_filename)\n",
    "\n",
    "#Specify model inputs: df, X, y, doc_labels\n",
    "df = data_dict['df_original']\n",
    "train_test_set = data_dict['train_test_sets']\n",
    "#Specify train-test_data for validation        \n",
    "Xy_doc_label_train = train_test_set[0][0]\n",
    "Xy_doc_label_test = train_test_set[0][1]\n",
    "X_train = train_test_set[0][2]\n",
    "X_test = train_test_set[0][3]\n",
    "y_train = train_test_set[0][4]\n",
    "y_test = train_test_set[0][5]\n",
    "\n",
    "#Define Model\n",
    "model = LogisticRegression(random_state=42)\n",
    "        # LogisticRegression(class_weight='balanced', random_state=42)\n",
    "        # LogisticRegression(solver='saga', penalty='elasticnet',\n",
    "                           #l1_ratio=0.25, C=0.5, random_state=42)\n",
    "#Fit model\n",
    "model.fit(X_train,y_train)\n",
    "#Predict Model\n",
    "y_pred = model.predict_proba(X_test)\n",
    "    \n",
    "#Convert to binary predictions\n",
    "y_pred_bin = (y_pred >=0.5)*1\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_bin[:,1], labels=[0,1])\n",
    "\n",
    "#Return predicted summaries\n",
    "idx, doc_index, pred_summaries = return_df_pred_summaries(Xy_doc_label_test, \n",
    "                                y_pred[:,1], df.text_clean, thresh=0.5, min_num=1, \n",
    "                                return_all = True, fix_num_flag=True, fix_num=3)\n",
    "\n",
    "#Match with gold summaries\n",
    "df_gold = df.summary_clean[doc_index]\n",
    "gold_summaries = [' '.join(df_gold .iloc[j]) for j in range(len(pred_summaries))]\n",
    "summaries_comp = tuple(zip(pred_summaries, gold_summaries))\n",
    "\n",
    "scores = calc_rouge_scores(pred_summaries, gold_summaries, \n",
    "                                  keys=['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results_dict ={'conf_matrix': cm, 'summaries_comp': summaries_comp,\n",
    "               'sent_index_number': idx, 'Rouge': scores}\n",
    "\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(results_dict, handle)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "t2 = dt.now()\n",
    "print(t2)\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.1.2__ Elastic Net Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "logreg_embeddings_only_gridsearch.py\n",
    "'''\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from functions import return_df_pred_summaries\n",
    "from functions import calc_rouge_scores\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "input_filename = 'train_test_set20_embeddings_only.pickle'             \n",
    "\n",
    "output_file = 'cv_results_top_3_logreg_elasticnet_gridsearch.pickle'\n",
    "\n",
    "\n",
    "t1 = dt.now()\n",
    "print(t1)\n",
    "\n",
    "data_dict = pd.read_pickle(input_filename)\n",
    "\n",
    "#Specify model inputs: df, X, y, doc_labels\n",
    "df = data_dict['df_original']\n",
    "train_test_set = data_dict['train_test_sets']\n",
    "#Specify train-test_data for validation        \n",
    "Xy_doc_label_train = train_test_set[0][0]\n",
    "Xy_doc_label_test = train_test_set[0][1]\n",
    "X_train = train_test_set[0][2]\n",
    "X_test = train_test_set[0][3]\n",
    "y_train = train_test_set[0][4]\n",
    "y_test = train_test_set[0][5]\n",
    "\n",
    "#Define Model\n",
    "l1_ratio_list = [0.25, 0.5, 0.75, 1]\n",
    "C_list = [0.25, 0.5, 1, 2, 4]\n",
    "\n",
    "rouge1_f1_list = []\n",
    "params_list = []\n",
    "\n",
    "for l1_ratio in l1_ratio_list:\n",
    "    for C in C_list:\n",
    "        \n",
    "        model = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=l1_ratio,\n",
    "                           C=C, random_state=42)\n",
    "        #Fit model\n",
    "        model.fit(X_train,y_train)\n",
    "        #Predict Model\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "    \n",
    "        #Convert to binary predictions\n",
    "        y_pred_bin = (y_pred >=0.5)*1\n",
    "   \n",
    "        #Return predicted summaries\n",
    "        idx, doc_index, pred_summaries = return_df_pred_summaries(Xy_doc_label_test, \n",
    "                                y_pred[:,1], df.text_clean, thresh=0.5, min_num=1, \n",
    "                                return_all = True, fix_num_flag=True, fix_num=3)\n",
    "\n",
    "        #Match with gold summaries\n",
    "        df_gold = df.summary_clean[doc_index]\n",
    "        gold_summaries = [' '.join(df_gold .iloc[j]) for j in range(len(pred_summaries))]\n",
    "        summaries_comp = tuple(zip(pred_summaries, gold_summaries))\n",
    "\n",
    "        scores = calc_rouge_scores(pred_summaries, gold_summaries, \n",
    "                                  keys=['rouge1', 'rougeL'], use_stemmer=True)\n",
    "        f1 = scores['rouge1']['f1']\n",
    "        rouge1_f1_list.append(f1)\n",
    "        \n",
    "        params = (l1_ratio, C)\n",
    "        params_list.append(params)\n",
    "        \n",
    "results_dict = {'params': params_list, 'scores': rouge1_f1_list}        \n",
    "\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(results_dict, handle)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "t2 = dt.now()\n",
    "print(t2)\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.2 Neural Net Models\n",
    " \n",
    "General Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''neural_network_embeddings_only'''\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from functions import return_df_pred_summaries\n",
    "from functions import calc_rouge_scores\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "input_filename = 'train_test_set20_embeddings_only.pickle'\n",
    "\n",
    "output_file = 'XXX.pickle'\n",
    "\n",
    "t1 = dt.now()\n",
    "print(t1)\n",
    "\n",
    "data_dict = pd.read_pickle(input_filename)\n",
    "\n",
    "#Specify model inputs: df, X, y, doc_labels\n",
    "df = data_dict['df_original']\n",
    "train_test_set = data_dict['train_test_sets']\n",
    "#Specify train-test_data for validation        \n",
    "Xy_doc_label_train = train_test_set[0][0]\n",
    "Xy_doc_label_test = train_test_set[0][1]\n",
    "X_train = train_test_set[0][2]\n",
    "X_test = train_test_set[0][3]\n",
    "y_train = train_test_set[0][4]\n",
    "y_test = train_test_set[0][5]\n",
    "\n",
    "#class_weights for imbalanced data\n",
    "pos_w = int(y_train.shape[0] / sum(y_train==1)[0])\n",
    "weight_dict = {0:1, 1: pos_w/2}\n",
    "   \n",
    "#Define Model\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=1536, activation='relu'))\n",
    "#model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#Compile Model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "metrics=[tf.keras.metrics.SensitivityAtSpecificity(0.5, num_thresholds=1),\n",
    "             tf.keras.metrics.SpecificityAtSensitivity(0.5, num_thresholds=1)])\n",
    "#Fit Model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32,\n",
    "                          callbacks=[callback], class_weight=weight_dict) #class_weight=weight_dict\n",
    "#Predict Model\n",
    "y_pred = model.predict(X_test)\n",
    "    \n",
    "#Convert to binary predictions\n",
    "y_pred_bin = (y_pred >=0.5)*1\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_bin, labels=[0,1])\n",
    "\n",
    "\n",
    "#Return predicted summaries\n",
    "idx, doc_index, pred_summaries = return_df_pred_summaries(Xy_doc_label_test, \n",
    "                                y_pred, df.text_clean, thresh=0.5, min_num=1, \n",
    "                                return_all = True, fix_num_flag=True, fix_num=3)\n",
    "\n",
    "#pred_summaries = [' '.join(df.text[doc_index].iloc[j][:3]) for j in range(len(idx))]\n",
    "\n",
    "#Match with gold summaries\n",
    "df_gold = df.summary_clean[doc_index]\n",
    "gold_summaries = [' '.join(df_gold .iloc[j]) for j in range(len(pred_summaries))]\n",
    "summaries_comp = tuple(zip(pred_summaries, gold_summaries))\n",
    "\n",
    "scores = calc_rouge_scores(pred_summaries, gold_summaries, \n",
    "                                  keys=['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "\n",
    "\n",
    "results_dict ={'conf_matrix': cm, 'summaries_comp': summaries_comp,\n",
    "               'sent_index_number': idx, 'Rouge': scores, 'mod_summary': model.summary()}\n",
    "\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(results_dict, handle)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "t2 = dt.now()\n",
    "print(t2)\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.3 TextRank\n",
    " \n",
    " __2.3.1__ Calculate ranking matrix per article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''textrank_ranking_matrix.py'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from datetime import datetime as dt\n",
    "\n",
    "t1 = dt.now()\n",
    "print(t1)\n",
    "\n",
    "output_file = 'train_stats_dict_processed_extr_final_5000_inc_pagerank.pickle' \n",
    "input_file = 'train_stats_dict_processed_extr_final_5000_.pickle' \n",
    "data = pd.read_pickle(input_file )\n",
    "\n",
    "#Select sentence embeddings only and match to doc label\n",
    "df_embed = data['df_X'].loc[:,'Sent_BERT_D_0': 'Sent_BERT_D_767']\n",
    "df_doc_label = pd.DataFrame(data['Xy_doc_label_array'],columns=['doc_label'])\n",
    "df = pd.concat([df_doc_label, df_embed], axis=1)\n",
    "\n",
    "#loop through articles (docs)\n",
    "pagerank_scores_list=[]\n",
    "error_list = []\n",
    "doc_num = np.max(data['Xy_doc_label_array']) \n",
    "for j in range(doc_num+1):\n",
    "    \n",
    "    #calculate cosine similiarity matrix \n",
    "    df_doc = df [df.doc_label == j].iloc[:,2:]\n",
    "    n = df_doc.shape[0]\n",
    "    cos_matrix = cosine_similarity(df_doc, df_doc)\n",
    "    f = np.vectorize(lambda x: 0 if x == 1 else 1)\n",
    "    not_eye = f(np.eye(n,n))\n",
    "    cos_matrix = cos_matrix * not_eye\n",
    "    \n",
    "    #Convert to nx graph\n",
    "    graph = nx.from_numpy_array(cos_matrix)\n",
    "    \n",
    "    #Calculate sentence scores and record error docs\n",
    "    try:\n",
    "        scores_arr = np.array(list(nx.pagerank(graph, max_iter=500).values()))\n",
    "    except:\n",
    "        scores_arr = np.nan\n",
    "        error_list.append(j)\n",
    "   \n",
    "    pagerank_scores_list.append(scores_arr)\n",
    "    \n",
    "pagerank_scores_arr = np.array(pagerank_scores_list)\n",
    "\n",
    "#store in primary dictionary\n",
    "data.update({'textrank_scores_arr_per_doc':pagerank_scores_arr })\n",
    "\n",
    "#save to pickle\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(data, handle)\n",
    "\n",
    "t2=dt.now()\n",
    "print(t2)\n",
    "print(t2-t1)\n",
    "\n",
    "#runtime 4mins50sec for 5000 docs / 29 errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.3.2__ Calculate TextRank Rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''textrank_rouge_score.py'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functions import calc_rouge_scores\n",
    "\n",
    "\n",
    "input_textrank = 'train_stats_dict_processed_extr_final_5000_inc_pagerank.pickle'\n",
    "input_test_labels = 'train_test_set20_embeddings_only.pickle'\n",
    "output_file = 'cv_results_textrank.pickle'\n",
    "\n",
    "data = pd.read_pickle(input_textrank )\n",
    "\n",
    "test_labels = pd.read_pickle(input_test_labels)\n",
    "test_labels = set(test_labels['train_test_sets'][0][1].flatten())\n",
    "\n",
    "#original df with columns including article / summary text\n",
    "df = data['df_original']\n",
    "#add pagerank scores to df\n",
    "df['textrank_scores'] = data['textrank_scores_arr_per_doc']\n",
    "#filter for test set\n",
    "df = df[df.index.isin(test_labels)]\n",
    "#drop where textrank had errors\n",
    "df = df.dropna()\n",
    "\n",
    "#pick top3 sentence by textrank score\n",
    "df['idx'] = df['textrank_scores'].apply(lambda x: sorted(np.argsort(x)[-3:])).values\n",
    "idx_arr = df['idx'].values\n",
    "\n",
    "#convert list of sentences to string for each predicted summary\n",
    "pred_summaries = [' '.join(np.array(df.text_clean.iloc[j])[idx_arr[j]].tolist())\n",
    "                  for j in range(len(idx_arr))]\n",
    "\n",
    "#convert cleaned gold summarysentence lists to string for each summary\n",
    "df_gold = df.summary_clean\n",
    "gold_summaries = [' '.join(df_gold .iloc[j]) for j in range(len(pred_summaries))]\n",
    "\n",
    "#zip each predicted / gold summary pair together and store in another tuple\n",
    "summaries_comp = tuple(zip(pred_summaries, gold_summaries))\n",
    "\n",
    "#calculate rouge scores\n",
    "scores = calc_rouge_scores(pred_summaries, gold_summaries, \n",
    "                                  keys=['rouge1', 'rougeL'], use_stemmer=True)\n",
    "#store results in dict\n",
    "results_dict = {'Rouge': scores, 'doc_labels': df.index.tolist(),\n",
    "                'summaries_comp': summaries_comp}\n",
    "#add to primary dict\n",
    "data.update(results_dict)\n",
    "\n",
    "#save to pickle\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(data, handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Supervised Learning Including Sequential Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.1 Logistic Regression Models\n",
    " \n",
    " __3.1.1__ General Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''logreg_embeddings_num_sent.py'''\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from functions import return_df_pred_summaries\n",
    "from functions import calc_rouge_scores\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "input_filename = 'train_test_set20_embeddings_sent_num.pickle'     \n",
    "\n",
    "output_file = 'XXX.pickle'\n",
    "\n",
    "t1 = dt.now()\n",
    "print(t1)\n",
    "\n",
    "data_dict = pd.read_pickle(input_filename)\n",
    "\n",
    "#Specify model inputs: df, X, y, doc_labels\n",
    "df = data_dict['df_original']\n",
    "train_test_set = data_dict['train_test_sets']\n",
    "#Specify train-test_data for validation        \n",
    "Xy_doc_label_train = train_test_set[0][0]\n",
    "Xy_doc_label_test = train_test_set[0][1]\n",
    "X_train = train_test_set[0][2]\n",
    "X_test = train_test_set[0][3]\n",
    "y_train = train_test_set[0][4]\n",
    "y_test = train_test_set[0][5]\n",
    "\n",
    "#Define Model\n",
    "model = LogisticRegression(random_state=42)\n",
    "        # LogisticRegression(class_weight='balanced', random_state=42)\n",
    "#Fit model\n",
    "model.fit(X_train,y_train)\n",
    "#Predict Model\n",
    "y_pred = model.predict_proba(X_test)\n",
    "    \n",
    "#Convert to binary predictions\n",
    "y_pred_bin = (y_pred >=0.5)*1\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_bin[:,1], labels=[0,1])\n",
    "\n",
    "   \n",
    "#Return predicted summaries\n",
    "idx, doc_index, pred_summaries = return_df_pred_summaries(Xy_doc_label_test, \n",
    "                                y_pred[:,1], df.text_clean, thresh=0.5, min_num=1, \n",
    "                                return_all = True, fix_num_flag=True, fix_num=3)\n",
    "\n",
    "\n",
    "#Match with gold summaries\n",
    "df_gold = df.summary_clean[doc_index]\n",
    "gold_summaries = [' '.join(df_gold .iloc[j]) for j in range(len(pred_summaries))]\n",
    "summaries_comp = tuple(zip(pred_summaries, gold_summaries))\n",
    "\n",
    "scores = calc_rouge_scores(pred_summaries, gold_summaries, \n",
    "                                  keys=['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "\n",
    "results_dict ={'conf_matrix': cm, 'summaries_comp': summaries_comp,\n",
    "               'sent_index_number': idx, 'Rouge': scores}\n",
    "\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(results_dict, handle)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "t2 = dt.now()\n",
    "print(t2)\n",
    "print(t2-t1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.2 Long Short Term Memory\n",
    " \n",
    "General Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''lstm1.py'''\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from functions import calc_rouge_scores\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "input_filename = 'train_test_set20_embeddings_only.pickle'\n",
    "output_file = 'XXX.pickle'\n",
    "\n",
    "data_dict = pd.read_pickle(input_filename)\n",
    "\n",
    "df = data_dict['df_original']\n",
    "\n",
    "#step 1: process data for ltsm input\n",
    "\n",
    "#convert to numpy array\n",
    "to_array = lambda x: np.array(x)\n",
    "df.text_embedding = df.text_embedding.apply(to_array)\n",
    "df.labels= df.labels.apply(to_array)\n",
    "df.text_embedding = df.text_embedding.apply(lambda x: x.reshape(1, x.shape[0],x.shape[1]))\n",
    "df.labels = df.labels.apply(lambda x: x.reshape(1, len(x),1))\n",
    "\n",
    "#train_test split\n",
    "train_doc_labels = set(data_dict['train_test_sets'][0][0].flatten())\n",
    "mask_train = np.array([x in train_doc_labels for x in df.index]) \n",
    "\n",
    "X_train = df.text_embedding[mask_train].tolist()\n",
    "y_train = df.labels[mask_train].tolist()\n",
    "\n",
    "X_test = df.text_embedding[~mask_train].tolist()\n",
    "y_test = df.labels[~mask_train].tolist()\n",
    "\n",
    "\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(25, input_shape=(None, 768), return_sequences=True, dropout=0))\n",
    "#model.add(LSTM(25, input_shape=(None, 768), return_sequences=True, dropout=0))\n",
    "\n",
    "#model.add(Bidirectional(LSTM(50, return_sequences=True, dropout=0), input_shape=(None, 768)))\n",
    "#model.add(Bidirectional(LSTM(50, return_sequences=True, dropout=0), input_shape=(None, 768)))\n",
    "\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=[tf.keras.metrics.SensitivityAtSpecificity(0.5, num_thresholds=1)])\n",
    "\n",
    "\n",
    "# train LSTM\n",
    "training_loss =[]\n",
    "training_metric = []\n",
    "for j in range(len(X_train)):   \n",
    "    X, y = X_train[j], y_train[j]\n",
    "    history = model.fit(X, y, epochs=1, batch_size=1)\n",
    "    training_loss.append(history.history['loss'])\n",
    "    \n",
    "# evaluate LSTM\n",
    "y_pred_list =[]\n",
    "idx_list=[]\n",
    "for j in range(len(X_test)):\n",
    "    X= X_test[j]\n",
    "    y_pred = model.predict(X, verbose=0)\n",
    "    idx = np.argsort(y_pred[0].flatten())[-3:]\n",
    "    idx = sorted(idx)\n",
    "    y_pred_list.append(y_pred)\n",
    "    idx_list.append(idx)\n",
    "\n",
    "    \n",
    "#retrieve summary pairs\n",
    "doc_index = df.index[~mask_train]\n",
    "pred_summaries = [' '.join(np.array(df.text_clean[doc_index].iloc[j])[np.array(idx_list[j])].tolist()) \n",
    "                  for j in range(len(idx_list))]\n",
    "df_gold = df.summary_clean[doc_index]\n",
    "gold_summaries = [' '.join(df_gold .iloc[j]) for j in range(len(pred_summaries))]\n",
    "summaries_comp = tuple(zip(pred_summaries, gold_summaries))\n",
    "\n",
    "\n",
    "#calculate rouge score\n",
    "scores = calc_rouge_scores(pred_summaries, gold_summaries, \n",
    "                                  keys=['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "results_dict ={'summaries_comp': summaries_comp,\n",
    "               'sent_index_number': idx, 'Rouge': scores, 'mod_summary': model.summary()}\n",
    "\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(results_dict, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.2 LEDE3\n",
    " \n",
    "Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LEDE3.py'''\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functions import calc_rouge_scores\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "input_filename = 'train_test_set20_embeddings_sent_num.pickle'             \n",
    "\n",
    "output_file = 'cv_results_LEDE3.pickle'\n",
    "\n",
    "t1 = dt.now()\n",
    "print(t1)\n",
    "\n",
    "data_dict = pd.read_pickle(input_filename)\n",
    "\n",
    "#Specify model inputs: df, X, y, doc_labels\n",
    "df = data_dict['df_original']\n",
    "\n",
    "#Specify train-test_data for validation        \n",
    "train_doc_labels = set(data_dict['train_test_sets'][0][0].flatten())\n",
    "mask_train = np.array([x in train_doc_labels for x in df.index]) \n",
    "mask_test = ~mask_train\n",
    "\n",
    "#Return predicted summary\n",
    "df_doc = df.text_clean[mask_test]\n",
    "    \n",
    "pred_summaries = [np.array(df_doc.iloc[j])[:3]\n",
    "                                          for j in range(len(df_doc))]\n",
    "\n",
    "pred_summaries = [summ_list if type(summ_list) == str else \n",
    "                      ' '.join(summ_list) for summ_list in pred_summaries]\n",
    "\n",
    "#Match with gold summaries\n",
    "df_gold = df.summary_clean[mask_test]\n",
    "gold_summaries = [' '.join(df_gold .iloc[j]) for j in range(len(pred_summaries))]\n",
    "summaries_comp = tuple(zip(pred_summaries, gold_summaries))\n",
    "\n",
    "scores = calc_rouge_scores(pred_summaries, gold_summaries, \n",
    "                                  keys=['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "results_dict ={'summaries_comp': summaries_comp,'Rouge': scores}\n",
    "\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(results_dict, handle)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "t2 = dt.now()\n",
    "print(t2)\n",
    "print(t2-t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
